{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX2qW5nu+ylpT2eS1wIBzp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexxpark/Projecy-LLM-/blob/main/WebscrapperWIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for carbonoffset\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_page(html):\n",
        "    soup = bs(html, 'html.parser')\n",
        "    title = soup.find('h1', {'id': 'firstHeading'}).get_text(strip=True)  # Extract the title\n",
        "    content = soup.find('div', {'id': 'mw-content-text'}).get_text(strip=True)  # Extract the content\n",
        "    return title, content\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove unwanted references and simplify whitespace\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation numbers like [1], [2], etc.\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def save_text(title, text, label, directory=\"data\"):\n",
        "    \"\"\"Save cleaned text to a file, labeled with '0' for PetroWiki.\"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)  # Ensure the directory exists\n",
        "    filename = os.path.join(directory, re.sub(r'[\\\\/:\"*?<>|]+', \"\", title) + \".txt\")  # Clean filename\n",
        "\n",
        "    # Formatting content for better readability and adding a label\n",
        "    content = f\"Label: {label}\\nTitle: {title}\\nContent: {text}\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "def main():\n",
        "    base_url = \"https://petrowiki.spe.org\"\n",
        "    start_page = \"/carbon_offset\"\n",
        "    url = f\"{base_url}{start_page}\"\n",
        "\n",
        "    print(f\"Fetching {url}\")\n",
        "    html = fetch_page(url)\n",
        "    if html:\n",
        "        title, content = parse_page(html)\n",
        "        cleaned_content = clean_text(content)\n",
        "        save_text(title, cleaned_content, label=\"0\")\n",
        "        time.sleep(1)  # Respectful scraping\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1371db18-a3eb-412a-a081-452380b828b5",
        "id": "jMFxmxaDJfjk"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://petrowiki.spe.org/Acid_fracturing\n",
            "Saved data/Acid fracturing.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for AARG\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_page(html):\n",
        "    soup = bs(html, 'html.parser')\n",
        "    title = soup.find('h1').get_text(strip=True)  # Extract the title assuming h1 is present for the title\n",
        "    content = soup.find('div', {'class': 'mw-parser-output'}).get_text(strip=True)  # Extract the content\n",
        "    return title, content\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove unwanted references and simplify whitespace\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation numbers like [1], [2], etc.\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def save_text(title, text, label, directory=\"data\"):\n",
        "    \"\"\"Save cleaned text to a file, labeled with '1' for AAPG Wiki.\"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)  # Ensure the directory exists\n",
        "    filename = os.path.join(directory, re.sub(r'[\\\\/:\"*?<>|]+', \"\", title) + \".txt\")  # Clean filename\n",
        "\n",
        "    # Formatting content for better readability and adding a label\n",
        "    content = f\"Label: {label}\\nTitle: {title}\\nContent: {text}\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "def main():\n",
        "    url = \"https://wiki.aapg.org/Carbon_dioxide_(CO2)_storage\"\n",
        "\n",
        "    print(f\"Fetching {url}\")\n",
        "    html = fetch_page(url)\n",
        "    if html:\n",
        "        title, content = parse_page(html)\n",
        "        cleaned_content = clean_text(content)\n",
        "        save_text(title, cleaned_content, label=\"1\")\n",
        "        time.sleep(1)  # Respectful scraping\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ovJPKoKSNeDf",
        "outputId": "0a0c60b5-2d69-4ed2-938e-670f6a0f1a6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://wiki.aapg.org/Carbon_dioxide_(CO2)_storage\n",
            "Saved data/Carbon dioxide (CO2) storage.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_page(html):\n",
        "    soup = bs(html, 'html.parser')\n",
        "    # Target the specific section by its 'id' in the 'span' within 'h2'\n",
        "    section_header = soup.find('span', {'id': 'Acid-fracturing_candidate_selection'})\n",
        "    if section_header:\n",
        "        # Get the parent 'h2' to access the subsequent paragraphs\n",
        "        section = section_header.parent\n",
        "        content = section.get_text(strip=True) + ' '  # Include the header in the text\n",
        "        # Get next siblings until it finds another 'h2' or runs out of siblings\n",
        "        for sibling in section.find_next_siblings():\n",
        "            if sibling.name == 'p':\n",
        "                content += sibling.get_text(strip=True) + ' '\n",
        "            else:\n",
        "                break\n",
        "        return content\n",
        "    return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove citation references and normalize whitespace\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation numbers like [1], [2], etc.\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def save_text(title, text, label, directory=\"data\"):\n",
        "    \"\"\"Save cleaned text to a file, labeled with '0' for PetroWiki.\"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)  # Ensure the directory exists\n",
        "    filename = os.path.join(directory, re.sub(r'[\\\\/:\"*?<>|]+', \"\", title) + \".txt\")  # Clean filename\n",
        "\n",
        "    # Formatting content for better readability and adding a label\n",
        "    content = f\"Label: {label}\\nTitle: {title}\\nContent: {text}\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "def main():\n",
        "    url = \"https://petrowiki.spe.org/Acid_fracturing\"\n",
        "\n",
        "    print(f\"Fetching {url}\")\n",
        "    html = fetch_page(url)\n",
        "    if html:\n",
        "        content = parse_page(html)\n",
        "        if content:\n",
        "            cleaned_content = clean_text(content)\n",
        "            save_text(\"Acid-fracturing_candidate_selection\", cleaned_content, label=\"0\")\n",
        "            time.sleep(1)  # Respectful scraping\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZtrfm1GPai1",
        "outputId": "c56f1621-deac-47d0-8015-ee6794854fc8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://petrowiki.spe.org/Acid_fracturing\n",
            "Saved data/Acid-fracturing_candidate_selection.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_page(html):\n",
        "    soup = bs(html, 'html.parser')\n",
        "    # Find all paragraphs in the desired section\n",
        "    paragraphs = soup.find_all('p')[:4]  # Adjust the index to select specific paragraphs\n",
        "    content = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
        "    return content\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove citation references and normalize whitespace\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation numbers like [1], [2], etc.\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def save_text(title, text, label, directory=\"data\"):\n",
        "    os.makedirs(directory, exist_ok=True)  # Ensure the directory exists\n",
        "    filename = os.path.join(directory, re.sub(r'[\\\\/:\"*?<>|]+', \"\", title) + \".txt\")  # Clean filename\n",
        "    # Formatting content for better readability and adding a label\n",
        "    content = f\"Label: {label}\\nTitle: {title}\\nContent: {text}\"\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "def main():\n",
        "    url = \"https://wiki.aapg.org/Basin-centered_gas\"\n",
        "    print(f\"Fetching {url}\")\n",
        "    html = fetch_page(url)\n",
        "    if html:\n",
        "        content = parse_page(html)\n",
        "        cleaned_content = clean_text(content)\n",
        "        save_text(\"Basin-centered_gas\", cleaned_content, label=\"1\")\n",
        "        time.sleep(1)  # Respectful scraping\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "okKbumNOP-mT",
        "outputId": "c4349f5d-e937-435c-83fc-237b8e6cb1c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://wiki.aapg.org/Basin-centered_gas\n",
            "Saved data/Basin-centered_gas.txt\n"
          ]
        }
      ]
    }
  ]
}