{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvC+EErX3JxLitbkvLuIaO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexxpark/Projecy-LLM-/blob/main/WebscrapperWIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_page(html):\n",
        "    soup = bs(html, 'html.parser')\n",
        "    content = soup.find_all(['p', 'h1', 'h2', 'h3', 'li'])  # Modify as necessary\n",
        "    return ' '.join([tag.get_text(strip=True) for tag in content])\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove HTML entities and unwanted spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in brackets\n",
        "    return text.strip()\n",
        "\n",
        "def save_text(title, text, directory=\"data\"):\n",
        "    \"\"\"Save cleaned text to a file, with the title separated visually from the body.\"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)  # Ensure the directory exists\n",
        "    filename = os.path.join(directory, re.sub(r'[\\\\/:\"*?<>|]+', \"\", title) + \".txt\")  # Clean filename\n",
        "\n",
        "    # Formatting content for better readability\n",
        "    content = f\" {title}\\n{text}\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    base_url = \"https://petrowiki.spe.org\"\n",
        "    start_page = \"/Carbon_offset\"\n",
        "    urls = [f\"{base_url}{start_page}\"]  # Start with the main topic page\n",
        "\n",
        "    for url in urls:\n",
        "        print(f\"Fetching {url}\")\n",
        "        html = fetch_page(url)\n",
        "        if html:\n",
        "            text = parse_page(html)\n",
        "            cleaned_text = clean_text(text)\n",
        "            page_title = url.split('/')[-1]  # Simplistic way to name the file\n",
        "            save_text(page_title, cleaned_text)\n",
        "        time.sleep(1)  # Respectful scraping\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_um2J0Hrvkf",
        "outputId": "27b0156e-7a33-4ef8-d920-27bf9e996ec2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://petrowiki.spe.org/Carbon_offset\n",
            "Saved data/Carbon_offset.txt\n"
          ]
        }
      ]
    }
  ]
}